# ğŸŒ PaperBanana

Implementation of the **PaperBanana** framework from ["PaperBanana: Automating Academic Illustration for AI Scientists"](hhttps://arxiv.org/abs/2601.23265) (Zhu et al., 2025).

> An agentic framework that turns methodology text into publication-ready architecture diagrams â€” no Figma, no TikZ, no tears.

## Examples

All images below were generated end-to-end by PaperBanana (3 refinement iterations each, using the NeurIPS 2025 spotlight reference set).

### Transformer â€” *Attention Is All You Need* (Vaswani et al., 2017)

<p align="center">
  <img src="examples/readme/transformer_iter3_0.jpg" width="600" />
</p>

### ResNet â€” *Deep Residual Learning* (He et al., 2016)

<p align="center">
  <img src="examples/readme/resnet_iter3_0.jpg" width="600" />
</p>

### DDPM â€” *Denoising Diffusion Probabilistic Models* (Ho et al., 2020)

<p align="center">
  <img src="examples/readme/ddpm_iter3_0.jpg" width="600" />
</p>

---

## How It Works

PaperBanana orchestrates **five specialized agents** in an iterative pipeline:

```
Methodology Text + Caption
        â”‚
        â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Retriever   â”‚  â†’ Finds relevant reference diagrams (from 100 NeurIPS examples)
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Planner    â”‚  â†’ Translates methodology into a detailed visual description
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Stylist    â”‚  â†’ Applies academic aesthetic guidelines
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Visualizer  â”‚  â†’ Generates the image (Gemini image generation)
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Critic     â”‚  â†’ Evaluates & provides feedback â†’ loops back to Planner
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Installation

```bash
pip install -r requirements.txt
```

Or with [uv](https://docs.astral.sh/uv/):

```bash
uv sync
```

## Setup

Create a `.env` file with your Gemini API key:

```
GEMINI_API_KEY=your-api-key-here
```

## Quick Start

```python
from paperbanana import generate_illustration
from load_reference_set import load_reference_set

# Load 100 curated NeurIPS 2025 architecture diagrams
ref_set = load_reference_set()

methodology = """
Our model uses a Vision Transformer backbone to extract patch embeddings,
followed by a cross-attention module that fuses text and image features.
The fused representation is decoded by a lightweight MLP head for classification.
"""

result = generate_illustration(
    methodology_text=methodology,
    caption="Architecture of our proposed vision-language fusion model",
    reference_set=ref_set,
    output_path="output/my_diagram",
)

print(f"Generated: {result['final_image_path']}")
```

### Advanced Usage

```python
from paperbanana import PaperBanana
from load_reference_set import load_reference_set

pb = PaperBanana(
    reference_set=load_reference_set(),
    mode="diagram",       # or "plot" for statistical plots
    max_iterations=3,
)

result = pb.generate(
    methodology_text=methodology,
    caption=caption,
    output_path="output/diagram",
)

# Save full generation history for analysis
pb.save_history("output/history.json")
```

## Project Structure

```
paperbanana/
â”œâ”€â”€ paperbanana.py            # Main orchestration
â”œâ”€â”€ config.py                 # API keys & model config
â”œâ”€â”€ aesthetic_guidelines.py   # NeurIPS-style visual guidelines
â”œâ”€â”€ utils.py                  # Shared utilities
â”œâ”€â”€ load_reference_set.py     # Load reference set for RetrieverAgent
â”œâ”€â”€ examples.py               # Runnable examples
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ retriever.py          # Retriever Agent  (VLM-based ranking)
â”‚   â”œâ”€â”€ planner.py            # Planner Agent    (methodology â†’ description)
â”‚   â”œâ”€â”€ stylist.py            # Stylist Agent    (aesthetic refinement)
â”‚   â”œâ”€â”€ visualizer.py         # Visualizer Agent (image generation)
â”‚   â””â”€â”€ critic.py             # Critic Agent     (evaluate & feedback)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ spotlight_reference_set.json      # 100 curated architecture diagrams
â”‚   â””â”€â”€ spotlight_reference_images/       # Corresponding images
â”œâ”€â”€ examples/                 # Generated output images
â”‚   â””â”€â”€ readme/               # Showcase examples shown above
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Reference Set

The Retriever Agent draws from **100 top-quality architecture diagrams** curated from NeurIPS 2025 Spotlight papers:

- **685** spotlight papers parsed with [MinerU](https://github.com/opendatalab/MinerU) on [Modal](https://modal.com) (50Ã— A10G GPUs)
- **1,732** methodology-section images extracted via section-aware filtering
- **321** verified architecture diagrams after 2-pass Gemini classification (caption + visual)
- **100** final diagrams selected by quality ranking (all scored 10/10)

## Configuration

Edit `config.py`:

| Setting | Default | Description |
|---------|---------|-------------|
| `VLM_MODEL` | `gemini-3-pro-preview` | Reasoning model (Retriever, Planner, Stylist, Critic) |
| `IMAGE_MODEL` | `gemini-3-pro-image-preview` | Image generation model (Visualizer) |
| `MAX_REFINEMENT_ITERATIONS` | `3` | Plannerâ†”Critic loop iterations |
| `NUM_REFERENCE_EXAMPLES` | `10` | References retrieved per generation |

## Paper

```bibtex
@article{zhu2025paperbanana,
  title={PaperBanana: Automating Academic Illustration for AI Scientists},
  author={Zhu, Dawei and Meng, Rui and Song, Yale and Wei, Xiyu and Li, Sujian and Pfister, Tomas and Yoon, Jinsung},
  journal={NeurIPS},
  year={2025}
}
```

## License

This implementation is for research and educational purposes.
